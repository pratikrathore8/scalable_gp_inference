{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b03b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_NAME = \"sketchy-opts\"\n",
    "PROJECT_NAME_BASE = \"gp_inference_\"\n",
    "METRIC_NAME_BASE = \"metrics.callback.\"\n",
    "METRIC_NAME_MAP = {\"test_rmse\": \"Test RMSE\", \"test_posterior_samples_mean_nll\": \"Test Mean NLL\", \"train_rmse\": \"Train RMSE\"}\n",
    "X_AXIS_NAME_MAP = {\"datapasses\": \"Datapasses\",\n",
    "                  \"iterations\": \"Iterations\", \"time\": \"Time (s)\"}\n",
    "BOUND_FILL = 0.2\n",
    "\n",
    "dataset = \"malonaldehyde\"\n",
    "metric = \"test_posterior_samples_mean_nll\"\n",
    "x_axis_name = \"time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ac0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = api.runs(f\"{ENTITY_NAME}/{PROJECT_NAME_BASE}{dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a143e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_in_latex():\n",
    "    plt.rcParams.update({\"text.usetex\": True, \"font.family\": \"serif\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e57579",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True, frozen=True)\n",
    "class MetricData:\n",
    "    \"\"\"Data class to hold metric data for a run.\"\"\"\n",
    "    metric_name: str\n",
    "    metric_data: np.ndarray\n",
    "    steps: np.ndarray\n",
    "    datapasses: np.ndarray\n",
    "    cum_times: np.ndarray\n",
    "    finished: bool\n",
    "\n",
    "    def get_final_time(self) -> float:\n",
    "        \"\"\"Return the last element in the cum_times array.\"\"\"\n",
    "        if len(self.cum_times) == 0:\n",
    "            raise ValueError(\"cum_times array is empty.\")\n",
    "        return self.cum_times[-1]\n",
    "\n",
    "    def get_plotting_name(self) -> str:\n",
    "        \"\"\"Return the name of the metric for plotting.\"\"\"\n",
    "        return self.metric_name\n",
    "    \n",
    "    def get_plotting_x_axis(self, xaxis) -> str:\n",
    "        if xaxis not in X_AXIS_NAME_MAP:\n",
    "            raise ValueError(f\"Invalid x-axis name: {xaxis}. Must be one of {X_AXIS_NAME_MAP}.\")\n",
    "        if xaxis == \"datapasses\":\n",
    "            return self.datapasses\n",
    "        elif xaxis == \"iterations\":\n",
    "            return self.steps\n",
    "        elif xaxis == \"time\":\n",
    "            return self.cum_times\n",
    "\n",
    "class WandbRun:\n",
    "    def __init__(self, run):\n",
    "        self.run = run\n",
    "\n",
    "    def _get_opt_name(self) -> str:\n",
    "        if self.run.config[\"solver_name\"] == \"sap\":\n",
    "            if self.run.config[\"solver_config\"].get(\"precond_config\", None):\n",
    "                return r\"\\texttt{ADASAP}\"\n",
    "            else:\n",
    "                return r\"\\texttt{ADASAP-I}\"\n",
    "        elif self.run.config[\"solver_name\"] == \"sdd\":\n",
    "            return f\"SDD-{self.run.config['opt_step_size_unscaled']}\"\n",
    "        elif self.run.config[\"solver_name\"] == \"pcg\":\n",
    "            if self.run.config[\"solver_config\"].get(\"precond_config\", None):\n",
    "                return \"PCG\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown solver name: {self.run.config['solver_name']}\")\n",
    "        \n",
    "    def _get_run_blksz(self) -> int:\n",
    "        if self.run.config[\"solver_name\"] in [\"sap\", \"sdd\"]:\n",
    "            return self.run.config[\"opt_num_blocks\"]\n",
    "        elif self.run.config[\"solver_name\"] == \"pcg\":\n",
    "            return 1\n",
    "    \n",
    "    @property\n",
    "    def opt_name(self) -> str:\n",
    "        return self._get_opt_name()\n",
    "        \n",
    "    def get_metric_data(self, metric: str) -> MetricData:\n",
    "        full_metric_name = f\"{METRIC_NAME_BASE}{metric}\"\n",
    "        run_hist = self.run.scan_history(\n",
    "            keys=[full_metric_name, \"_step\", \"iter_time\"])\n",
    "\n",
    "        # Extract raw data\n",
    "        metric_data = np.array([x[full_metric_name] for x in run_hist])\n",
    "        steps = np.array([x[\"_step\"] for x in run_hist])\n",
    "        times = np.array([x[\"iter_time\"] for x in run_hist])\n",
    "\n",
    "        # Identify unique step indices -- this is needed to remove duplicates\n",
    "        _, unique_indices = np.unique(steps, return_index=True)\n",
    "        unique_indices = np.sort(unique_indices)  # Sort to maintain original order\n",
    "\n",
    "        # Filter to keep only unique step entries\n",
    "        metric_data = metric_data[unique_indices]\n",
    "        times = times[unique_indices]\n",
    "        steps = steps[unique_indices]\n",
    "        \n",
    "        num_blocks = self._get_run_blksz()\n",
    "        datapasses = steps / num_blocks\n",
    "\n",
    "        # Calculate cumulative times\n",
    "        cum_times = np.cumsum(times)\n",
    "\n",
    "        return MetricData(\n",
    "            metric_name=METRIC_NAME_MAP[metric],\n",
    "            metric_data=metric_data,\n",
    "            steps=steps,\n",
    "            datapasses=datapasses,\n",
    "            cum_times=cum_times,\n",
    "            finished=True if self.run.state == \"finished\" else False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb98a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_metric_statistics(\n",
    "    metrics_list: list[MetricData],\n",
    ") -> tuple[MetricData, MetricData, MetricData]:\n",
    "    \"\"\"\n",
    "    Compute mean, minimum, and maximum for a list of MetricData objects.\n",
    "    \n",
    "    Args:\n",
    "        metrics_list: List of MetricData objects\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (mean_data, min_data, max_data)\n",
    "    \"\"\"\n",
    "    # Check if all metrics have the same metric name\n",
    "    reference = metrics_list[0]\n",
    "    all_same_name = all(m.metric_name == reference.metric_name\n",
    "                        for m in metrics_list)\n",
    "    if not all_same_name:\n",
    "        raise ValueError(\n",
    "            \"All MetricData objects must have the same metric name\")\n",
    "\n",
    "    # Check if all metrics have the same steps\n",
    "    reference = metrics_list[0]\n",
    "    all_same_steps = all(np.array_equal(m.steps, reference.steps)\n",
    "                         for m in metrics_list)\n",
    "\n",
    "    if not all_same_steps:\n",
    "        raise ValueError(\"All MetricData objects must have the same steps\")\n",
    "\n",
    "    # Stack metric data along a new axis\n",
    "    stacked_metrics = np.stack([m.metric_data for m in metrics_list], axis=0)\n",
    "\n",
    "    # Stack cum_times data\n",
    "    stacked_cum_times = np.stack([m.cum_times for m in metrics_list], axis=0)\n",
    "\n",
    "    # Compute means\n",
    "    mean_metrics = np.mean(stacked_metrics, axis=0)\n",
    "    mean_cum_times = np.mean(stacked_cum_times, axis=0)\n",
    "\n",
    "    # Find actual min and max values\n",
    "    min_metrics = np.min(stacked_metrics, axis=0)\n",
    "    max_metrics = np.max(stacked_metrics, axis=0)\n",
    "\n",
    "    # Check if all runs are finished\n",
    "    all_finished = all(m.finished for m in metrics_list)\n",
    "\n",
    "    # Create MetricData objects for mean, min, and max\n",
    "    mean_data = MetricData(\n",
    "        metric_data=mean_metrics,\n",
    "        steps=reference.steps,\n",
    "        datapasses=reference.datapasses,\n",
    "        cum_times=mean_cum_times,\n",
    "        finished=all_finished,\n",
    "        metric_name=reference.metric_name\n",
    "    )\n",
    "\n",
    "    min_data = MetricData(\n",
    "        metric_data=min_metrics,\n",
    "        steps=reference.steps,\n",
    "        datapasses=reference.datapasses,\n",
    "        cum_times=mean_cum_times,  # Using the same mean cum_times for all\n",
    "        finished=all_finished,\n",
    "        metric_name=reference.metric_name\n",
    "    )\n",
    "\n",
    "    max_data = MetricData(\n",
    "        metric_data=max_metrics,\n",
    "        steps=reference.steps,\n",
    "        datapasses=reference.datapasses,\n",
    "        cum_times=mean_cum_times,  # Using the same mean cum_times for all\n",
    "        finished=all_finished,\n",
    "        metric_name=reference.metric_name\n",
    "    )\n",
    "\n",
    "    return mean_data, min_data, max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "for run in runs:\n",
    "    run_obj = WandbRun(run)\n",
    "    opt_name = run_obj.opt_name\n",
    "    if opt_name not in metrics_dict:\n",
    "        metrics_dict[opt_name] = []\n",
    "\n",
    "    metric_data = run_obj.get_metric_data(metric)\n",
    "    metrics_dict[opt_name].append(metric_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe19022",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_in_latex()\n",
    "\n",
    "min_final_time = np.inf\n",
    "xlims = (np.inf, -np.inf)\n",
    "for i, (opt_name, metric_data_list) in enumerate(list(metrics_dict.items())):\n",
    "    mean_data, lower_bound_data, upper_bound_data = _compute_metric_statistics(\n",
    "        metric_data_list\n",
    "    )\n",
    "\n",
    "    # Extract name for y-axis\n",
    "    if i == 0:\n",
    "        metric_name = mean_data.metric_name\n",
    "\n",
    "    # Plot the mean and bounds if the runs were all finished\n",
    "    if mean_data.finished:\n",
    "        x_axis = mean_data.get_plotting_x_axis(x_axis_name)\n",
    "        plt.plot(x_axis, mean_data.metric_data, label=opt_name)\n",
    "        plt.fill_between(\n",
    "            x_axis,\n",
    "            lower_bound_data.metric_data,\n",
    "            upper_bound_data.metric_data,\n",
    "            alpha=BOUND_FILL,\n",
    "        )\n",
    "\n",
    "        # Update xlims\n",
    "        xlims = (min(xlims[0], x_axis[0]), max(xlims[1], x_axis[-1]))\n",
    "\n",
    "        # Track the final time for the x-axis limit\n",
    "        min_final_time = min(min_final_time, mean_data.get_final_time())\n",
    "\n",
    "# For time-based x-axis, set the second xlim to the minimum final time\n",
    "if x_axis_name == \"time\":\n",
    "    xlims = (xlims[0], min_final_time)\n",
    "\n",
    "plt.xlim(xlims)\n",
    "plt.xlabel(X_AXIS_NAME_MAP[x_axis_name])\n",
    "plt.ylabel(metric_name)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp_inference_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
